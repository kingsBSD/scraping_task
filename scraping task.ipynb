{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond the Standard Library!\n",
    "\n",
    "Python's standard library is very powerfull, but sometimes we need more. We will scrape the Internet Archive to get data on the progress of Jill Stein's [crowdfunding campaign](http://jillstein.nationbuilder.com/recount) for recounts in the 2016 U.S. presidential election. We're also going to look at some data from\n",
    "a [survey on fanfiction](https://kingsbsd.github.io/scraping_task/).\n",
    "\n",
    "We're going to use the\n",
    "[Requests](http://docs.python-requests.org/en/master/)' library to download the data, then analyse it\n",
    "with [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) and the\n",
    "[Natural Language Tool-Kit](http://www.nltk.org/book/) (NLTK), and display it using [MatPlotLib](http://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard library:\n",
    "from datetime import datetime\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "# https://www.crummy.com/software/BeautifulSoup/\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# http://matplotlib.org/\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "\n",
    "# http://www.nltk.org/book/\n",
    "import nltk\n",
    "\n",
    "# http://docs.python-requests.org/en/master/\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up: (X-)File IO\n",
    "\n",
    "The next cell will use the standard library to download an\n",
    "[episode guide to the X-Files](http://www.textfiles.com/media/xfepgd.txt) from [www.textfiles.com](http://www.textfiles.com/). It\n",
    "contains a lot of text, including short lists of episodes for each season. Your job is to extract these lists\n",
    "and save them to another file. Each line in the lists starts with a code, `2X13` for example. The function\n",
    "`get_x_code` will return a code if a line of text starts with a code, or `False` if it doesn't. -It uses regular\n",
    "expressions, don't worry about them, (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('http://www.textfiles.com/media/xfepgd.txt', 'xfepgd.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_x_code(line):\n",
    "    matches = re.findall('^[1-3]X[0-9]{1,2}',line)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a quick look at the Python documentation for\n",
    "[reading and writing files](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files). The\n",
    "next cell shows an example of opening the file `xfepgd.txt` for reading and iterating over each line. Modify\n",
    "it so that `episodes` is popluated with each line that contains an episode starting with a code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episodes = []\n",
    "with open('xfepgd.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you need to do now is modify the code slightly to *write* the `episodes` list to a file\n",
    "called `x-files-episodes.txt`. The first two lines of the file should be the `headers` list. Use the `write`\n",
    "method of a file object. You should see your new file in the notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = ['Code Episode                        Air Date  Rebroadcasts',\n",
    "    '---- -------                        --------  ------------']\n",
    "# You're own your own now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Jill Stein Recount\n",
    "\n",
    "Look up [jillstein.nationbuilder.com/recount](http://jillstein.nationbuilder.com/recount) on the\n",
    "[Internet Archive](https://archive.org/web/). There are lots of snapshots from November 2016, and we can track how\n",
    "the ammount of money raised and requested evolved with time. It would be nice to be able to automate this.\n",
    "The [Internet Archive](https://archive.org/web/) provides us with an\n",
    "[API](https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server) for getting data on the snapshots\n",
    "the archive holds for a given web-page. We'll access\n",
    "it using the Requests library. We'll create a `request` object, and use it's `.json()` method to return all the JSON data\n",
    "(JavaScript Object Notation) from the API call as standard Python collections.\n",
    "\n",
    "Here's what it should look like:\n",
    "\n",
    "```\n",
    "[['urlkey',\n",
    "  'timestamp',\n",
    "  'original',\n",
    "  'mimetype',\n",
    "  'statuscode',\n",
    "  'digest',\n",
    "  'length'],\n",
    " ['com,nationbuilder,jillstein)/recount',\n",
    "  '20161123205129',\n",
    "  'https://jillstein.nationbuilder.com/recount',\n",
    "  'text/html',\n",
    "  '200',\n",
    "  'W6HBNX5QXRX4X3OGHN6VSUTSMIKVOLSY',\n",
    "  '13059'],\n",
    " ['com,nationbuilder,jillstein)/recount',\n",
    "  '20161123211323',\n",
    "  'https://jillstein.nationbuilder.com/recount',\n",
    "  'text/html',\n",
    "  '200',\n",
    "  '32B2DRBBQ525UHKR3KFA5G56YRTUZV2L',\n",
    "  '15287']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "req = requests.get(\"https://web.archive.org/cdx/search/cdx?url=jillstein.nationbuilder.com/recount&output=json\")\n",
    "req.json()[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Timestamps\n",
    "\n",
    "We get a list of lists, the first one is a list of headings for all the other lists, which contain the data. We\n",
    "need the timestamp of each snapshot. Build a list called `timestamps`, from the second list onwards, the timestamp \n",
    "will be the second item. Use a `for` loop if you like. Here's what the first 5 should look like:\n",
    "\n",
    "```\n",
    "['20161123205129', '20161123211323', '20161123234049', '20161123234501', '20161124001441']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestamps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(timestamps[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Snapshots\n",
    "\n",
    "We need to download the snapshots at each timestamp. The URL of the first snapshot is:\n",
    "\n",
    "```\n",
    "http://web.archive.org/web/20161123205129/https://jillstein.nationbuilder.com/recount\n",
    "```\n",
    "\n",
    "Complete the `web_archive_request` function so that it returns a `request` object for a given timestamp using\n",
    "the Request library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def web_archive_request(timestamp, url='https://jillstein.nationbuilder.com/recount'):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `content` of the `request` for the first timestamp should be a big mess of HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recount_1 = web_archive_request('20161123205129').content\n",
    "print(recount_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of the\n",
    "[snapshots](https://web.archive.org/web/20161129012122/https://jillstein.nationbuilder.com/recount). The raised\n",
    "and goal amounts are contained in pairs of div tags (`<div></div>`) with the CSS classes of `bar-text` and\n",
    "`bar-goal`. (Try \"view source\" in your browser) We can use the BeautifulSoup library to extract this text. Here it is for the first snapshot:\n",
    "    \n",
    "```\n",
    "$87,122.13 raised GOAL: $2,500,000.00\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recount_soup_1 = BeautifulSoup(recount_1, 'html.parser')\n",
    "raised_1 = recount_soup_1.find(class_='bar-text')\n",
    "goal_1 = recount_soup_1.find(class_='bar-goal')\n",
    "print(raised_1.text, goal_1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Values from Text\n",
    "\n",
    "We need to turn the texts into floating point values. Complete the function `get_dollar_ammount`.\n",
    "`float('87122.13')` works but the commas in `float('87,122.13')` will make it fail. We need to get rid of\n",
    "everything but the numbers and the decimal point. Would the `split()` and `join()` functions help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dollar_ammount(text):\n",
    "    return float(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_dollar_ammount(raised_1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Data\n",
    "\n",
    "We've almost got everything we need to scrape the data and display it. Complete the function `scrape_ammounts`\n",
    "which takes the list of timestamps and returns the lists of raised and goal ammounts in a tuple. Create a\n",
    "BeautifulSoup object from the content of request objects for each timestamp. Get the contents of the div tags\n",
    "with the classes `bar-text` and `bar-goal`. You've already written all the functions you need, and the examples\n",
    "of using the `find` method of a BeautifulSoup have already been given. Not all of the snapshots will contain the\n",
    "right data, so you need to use try-except blocks to avoid runtime errors. Here are the first 5 data-points:\n",
    "    \n",
    "```\n",
    "[87122.13, 131526.2, 626916.47, 646386.47, 780759.0]\n",
    "[2500000.0, 2500000.0, 2500000.0, 2500000.0, 2500000.0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_ammounts(times):\n",
    "    raised = []\n",
    "    goal = []\n",
    "    for t in times:\n",
    "        soup = None\n",
    "        try:\n",
    "            raised.append()\n",
    "        except:\n",
    "            #print(\"Can't get raised ammount for timestamp: \"+t)\n",
    "            pass\n",
    "        try:             \n",
    "            goal.append()\n",
    "        except:\n",
    "            #print(\"Can't get goal ammount for timestamp: \"+t)\n",
    "            pass\n",
    "    return raised,goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raised, goal = scrape_ammounts(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(raised[0:5])\n",
    "print(goal[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Data\n",
    "\n",
    "You might not have written the most concise, elegant code, but that would have taken longer. Besides, we didn't\n",
    "want code, we wanted data. Matplotlib is hard to use, so just run the cell below to plot a graph and see how the\n",
    "amount raised responded to the ammount requested. The Internet Archive, Requests library and  BeautifulSoup are\n",
    "a powerfull combination. What would *you* do with them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "n = len(raised)\n",
    "times = [datetime.strptime(t, '%Y%m%d%H%M%S') for t in timestamps[0:n]]\n",
    "raised_millions = [i/1E6 for i in raised]\n",
    "goal_millions = [i/1E6 for i in goal]\n",
    "fig, ax = plt.subplots()\n",
    "fmt = md.DateFormatter('%m/%d %H:%M')\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "plt.xticks(rotation=25)\n",
    "raised_line, = plt.plot(times, raised_millions, marker='*')\n",
    "ax.set_xlabel('Time Retrieved')\n",
    "ax.set_ylabel('Amount in Millions of $')\n",
    "goal_line, = plt.plot(times, goal_millions)\n",
    "plt.legend([raised_line, goal_line], ['raised','goal'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "survey_url = 'https://kingsbsd.github.io/scraping_task/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a Fan Fiction Survey\n",
    "\n",
    "Take a look at this [survey](https://kingsbsd.github.io/scraping_task/) of read and written Fan Fiction. Some\n",
    "people responded with neat, comma-separated lists of titles. Others have embedded the titles in free-form text.\n",
    "There will be spelling and punctuation errors. The idea is to extract titles from the survey, without picking up\n",
    "too much junk. We will try a very simple Natural Language processing (NLP) approach. It won't be perfect, but it'll\n",
    "do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the contents of a web page\n",
    "Store the contents of the web page in the string \"fan_doc\". Call requests' \"get\" method to obtain a response object, and use its \"text\" attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "survey_url = 'https://kingsbsd.github.io/scraping_task/'\n",
    "fan_doc = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"fan_doc\" should look like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<head>\n",
    "<title>Fandom Popularity in 2004</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Fanfiction Questionnaire Results: Fandoms</h1>\n",
    "<h2>Q2_13: Which fandoms or what type of original amateur fiction do you <em>read</em>?</h2>\n",
    "<p><table border='1' cellpadding='5' cellspacing='0'>\n",
    "<tr><td>Star Trek, Stargate, Sherlock Holmes, Harry Potter, Star Wars, Gundam Wing, X--Files, Lord of the Rings</td><td>1</td></tr>\n",
    "<tr><td>Buffy/Angel, Harry Potter, Forever Knight, Andromeda, Highlander (currently in a Horsemen-only phase), some 'older' anime every now and then (slayers, weiss kreuz, fushigi yuugi, video game)</td><td>1</td></tr>\n",
    "<tr><td>I will read GEN fic in all of the fandom that I like. I read only GEN in Lord of the Rings, and Kung Fu: The Ledgend Continues.  &lt;br /&gt;I read SLASH (in addition to GEN) in the following: Gundam Wing, Yu Yu Hakusho, Highlander, Smallville, Dogma, Pirates of the Carribbean, Star Trek TOS, Star Trek TNG, Stargate SG-1, The Matrix, The Vampire Chronicels, Xena and Hercules (but only about the gods), and X-Men.&lt;br /&gt;I read HET (in addition to GEN) in the X-Files fandom. I do not read X-Files slash.&lt;br /&gt;&lt;br /&gt;However, many fandoms tend to swing wildly out of character in fanfiction. For example, Lord of the Rings and Gundam Wing. I DO NOT read anything that strays too far out of character, so when fandoms start to 'age' and get filled up with people who don't know much about the characters (often they only know about the characters from fanfiction, having seen very little, if anything, of the actual shows) I tend to drift away rather than search through piles of crappy, OOC fic to find one good one. </td><td>1</td></tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fan_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "We now create a BeautifulSoup object, so we can extract the text we want from the HTML mark-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fan_soup = BeautifulSoup(fan_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BeautifulSoup object should already be a lot more readable:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<html><head>\n",
    "<title>Fandom Popularity in 2004</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Fanfiction Questionnaire Results: Fandoms</h1>\n",
    "<h2>Q2_13: Which fandoms or what type of original amateur fiction do you <em>read</em>?</h2>\n",
    "<p><table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
    "<tbody><tr><td>Star Trek, Stargate, Sherlock Holmes, Harry Potter, Star Wars, Gundam Wing, X--Files, Lord of the Rings</td><td>1</td></tr>\n",
    "<tr><td>Buffy/Angel, Harry Potter, Forever Knight, Andromeda, Highlander (currently in a Horsemen-only phase), some 'older' anime every now and then (slayers, weiss kreuz, fushigi yuugi, video game)</td><td>1</td></tr>\n",
    "<tr><td>I will read GEN fic in all of the fandom that I like. I read only GEN in Lord of the Rings, and Kung Fu: The Ledgend Continues.  &lt;br /&gt;I read SLASH (in addition to GEN) in the following: Gundam Wing, Yu Yu Hakusho, Highlander, Smallville, Dogma, Pirates of the Carribbean, Star Trek TOS, Star Trek TNG, Stargate SG-1, The Matrix, The Vampire Chronicels, Xena and Hercules (but only about the gods), and X-Men.&lt;br /&gt;I read HET (in addition to GEN) in the X-Files fandom. I do not read X-Files slash.&lt;br /&gt;&lt;br /&gt;However, many fandoms tend to swing wildly out of character in fanfiction. For example, Lord of the Rings and Gundam Wing. I DO NOT read anything that strays too far out of character, so when fandoms start to 'age' and get filled up with people who don't know much about the characters (often they only know about the characters from fanfiction, having seen very little, if anything, of the actual shows) I tend to drift away rather than search through piles of crappy, OOC fic to find one good one. </td><td>1</td></tr>\n",
    "<tr><td>Fandoms: CSI, Lotr RPS, The O.C. Homicide, X-Files, The Sentinel, Highlander, Sports Night, West Wing</td><td>1</td></tr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fan_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the desired content\n",
    "Most of the the methods of a BeautifulSoup object return another BeautifulSoup object. We can chain these method calls\n",
    "together to drill down into HTML document's structure until we get what we're looking for. Here, we get all the table row\n",
    "(`<tr>`) elements in the first table in the page with the \"find_all method\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fan_rows = fan_soup.find('table').find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fan_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function \"get_row_text\", so that when passed a  BeautifulSoup object for a row element it returns the\n",
    "text contents of its *first* `<td>` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_row_text(row):\n",
    "    #FIXME:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `<td>` element of the second `<tr>` element should look like:\n",
    "    \n",
    "```Buffy/Angel, Harry Potter, Forever Knight, Andromeda, Highlander (currently in a Horsemen-only phase),\n",
    " some 'older' anime every now and then (slayers, weiss kreuz, fushigi yuugi, video game)```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_row_text(fan_rows[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll combine the content of all the rows into a big list. This is called a [\"list comprehension\"](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions). Sometimes\n",
    "they can be a lot more readable and concise than appending to lists in a for loop. Don't worry about them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fandoms = [get_row_text(row) for row in fan_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting fiction titles with NLTK\n",
    "Named Entity Recognition (NER) can be hard, but sometimes we can do quite well with a simple approach. Many of the\n",
    "titles in the \"fandoms\" list will be badly typed, but most will follow the\n",
    "[accepted rules](http://grammar.yourdictionary.com/capitalization/rules-for-capitalization-in-titles.html)\n",
    "for capitalization, and we can make use of this. Some of the texts will still contain HTML, so we'll strip it out\n",
    "with BeautifulSoup again. Then we'll use NLTK to chop them into lists of words or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_clean_words(txt):\n",
    "    soup = BeautifulSoup(txt)\n",
    "    return [str(word) for word in nltk.word_tokenize(soup.text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how we get rid of a rogue `<br />` tag in the 11th text:\n",
    "    \n",
    "```\n",
    "\n",
    "Don't read original.<br />Eroica, Garrison's Gorillas, x-files, Harry Potter, Pirates of the CAribbean, Lord of the Rings, Master & commander, Smallville, Buffy, Wild Wild West, Wiseguy, Quantum Leap and a multitude of others.\n",
    "\n",
    "['Do', \"n't\", 'read', 'original.Eroica', ',', 'Garrison', \"'s\", 'Gorillas', ',', 'x-files', ',', 'Harry', 'Potter', ',', 'Pirates', 'of', 'the', 'CAribbean', ',', 'Lord', 'of', 'the', 'Rings', ',', 'Master', '&', 'commander', ',', 'Smallville', ',', 'Buffy', ',', 'Wild', 'Wild', 'West', ',', 'Wiseguy', ',', 'Quantum', 'Leap', 'and', 'a', 'multitude', 'of', 'others', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fandoms[10])\n",
    "print()\n",
    "print(get_clean_words(fandoms[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have another list comprhension to strip all the HTML out of \"fandoms\" and put the result in \"clean_fandoms\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_fandoms = [get_clean_words(f) for f in fandoms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function \"chunker\" that takes lists of words and punctuation, and returns lists of lists of\n",
    "consecutive words with all the punctuation removed. Here's the algorithm:\n",
    "\n",
    "* Start with a list of lists that contains one empty list.\n",
    "* Iterate over the input list of words and punctuation.\n",
    "  + If the word is alpha-numeric (`word.isaplha()`) then append it to the last list in the list of lists. Remember\n",
    "    about [list slicing](https://docs.python.org/3/tutorial/introduction.html#lists).\n",
    "  + Otherwise, append a new empty list to the list of lists.\n",
    "                         "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def chunker(words):\n",
    "    chunk_lists = [[]]\n",
    "    for w in words:\n",
    "        if True: #FIXME\n",
    "            pass #FIXME\n",
    "        else:\n",
    "            if True: #FIXME\n",
    "                pass # FIXME\n",
    "    return chunk_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what you should get for the 14th text. It looks pretty good already, but we've broken\n",
    "\"Xena, the worrior princes\" (sic) into two. You really can't win them ALL in this game!\n",
    "\n",
    "```\n",
    "[['Lord', 'of', 'the', 'Rings'], ['Queer', 'as', 'Folk'], ['Buffy', 'The', 'Vampire', 'Slayer'], ['Harry', 'Potter'], ['Babylon', 'Five'], ['Xena'], ['The', 'worrior', 'princes'], ['Some', 'different', 'original', 'slash'], ['I', 'do', 'read', 'Het'], ['but', 'I', 'prefer', 'slash'], []]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(chunker(clean_fandoms[13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of short words we often find in fiction titles that don't need to be capitalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shortwords = ['a', 'an', 'as', 'the', 'and', 'but', 'or', 'for', 'on', 'at', 'to', 'from', 'by', 'of']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the \"find_titles\" function. We'll use a very similar algorithm to extract titles from our lists of words. A title must start and end on a capitalized word, and may contain only capitalized words, or words from the \"stopwords\" list we pass in.\n",
    "\n",
    "* \"title_list\" is a list of lists, that contains one empty list.\n",
    "* For each word:\n",
    "  + Append the word to the last list in \"title_list\" IF:\n",
    "    - The 1st character is [uppercase](https://docs.python.org/3/library/stdtypes.html#str.isupper).\n",
    "    - OR the word is in \"stopwords\" AND the last list has at least one word in it.\n",
    "  + ELSE append a new empty list to \"title_list\".\n",
    "* Join all the words in the lists together with spaces. (This is done for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_titles(words, stopwords=shortwords):\n",
    "    title_list = [[]]\n",
    "    for w in words:\n",
    "        if True or (True and True):   \n",
    "            pass\n",
    "        else:\n",
    "            if True:\n",
    "                pass\n",
    "\n",
    "    titles = []\n",
    "    \n",
    "    for l in title_list:\n",
    "        if l:\n",
    "            titles.append(' '.join(l))\n",
    "                                    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_titles(fandom, stopwords=shortwords):\n",
    "    titles = []\n",
    "    for word_list in chunker(fandom):\n",
    "        titles.append(find_titles(word_list, stopwords=stopwords))        \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the previous function on the 3rd and 14th texts it looks pretty reasonable, we've got rid of a lot of the junk. It's still\n",
    "not perfect:\n",
    " \n",
    "```[['I', 'GEN', 'I'], ['I', 'GEN', 'Lord of the Rings'], ['Kung Fu'], ['The Ledgend Continues'], ['I', 'SLASH'], ['GEN'], [], ['Gundam Wing'], ['Yu Yu Hakusho'], ['Highlander'], ['Smallville'], ['Dogma'], ['Pirates of the Carribbean'], ['Star Trek TOS'], ['Star Trek TNG'], ['Stargate'], ['The Matrix'], ['The Vampire Chronicels'], ['Xena and Hercules'], [], [], ['HET'], ['GEN'], [], [], ['I'], [], ['For'], ['Lord of the Rings and Gundam Wing'], ['I DO NOT'], [], [], [], [], [], [], [], ['I'], ['OOC'], []]```\n",
    "\n",
    "```[['Lord of the Rings'], ['Queer as Folk'], ['Buffy The Vampire Slayer'], ['Harry Potter'], ['Babylon Five'], ['Xena'], ['The'], ['Some'], ['I', 'Het'], ['I'], []]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(extract_titles(clean_fandoms[2]))\n",
    "print()\n",
    "print(extract_titles(clean_fandoms[13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the advantages and disadvantages of including \"and\" in the stopwords? Look at texts 3 and 6:\n",
    "\n",
    "```[['I', 'GEN', 'I'], ['I', 'GEN', 'Lord of the Rings'], ['Kung Fu'], ['The Ledgend Continues'], ['I', 'SLASH'], ['GEN'], [], ['Gundam Wing'], ['Yu Yu Hakusho'], ['Highlander'], ['Smallville'], ['Dogma'], ['Pirates of the Carribbean'], ['Star Trek TOS'], ['Star Trek TNG'], ['Stargate'], ['The Matrix'], ['The Vampire Chronicels'], ['Xena', 'Hercules'], [], [], ['HET'], ['GEN'], [], [], ['I'], [], ['For'], ['Lord of the Rings', 'Gundam Wing'], ['I DO NOT'], [], [], [], [], [], [], [], ['I'], ['OOC'], []]\n",
    "```\n",
    "\n",
    "```\n",
    "[['Mostly Harry Potter', 'Lord of the Rings'], [], []]\n",
    "```\n",
    "\n",
    "```\n",
    "[['I', 'GEN', 'I'], ['I', 'GEN', 'Lord of the Rings'], ['Kung Fu'], ['The Ledgend Continues'], ['I', 'SLASH'], ['GEN'], [], ['Gundam Wing'], ['Yu Yu Hakusho'], ['Highlander'], ['Smallville'], ['Dogma'], ['Pirates of the Carribbean'], ['Star Trek TOS'], ['Star Trek TNG'], ['Stargate'], ['The Matrix'], ['The Vampire Chronicels'], ['Xena and Hercules'], [], [], ['HET'], ['GEN'], [], [], ['I'], [], ['For'], ['Lord of the Rings and Gundam Wing'], ['I DO NOT'], [], [], [], [], [], [], [], ['I'], ['OOC'], []]\n",
    "```\n",
    "\n",
    "```\n",
    "[['Mostly Harry Potter and Lord of the Rings'], [], []]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_and = ['a', 'an', 'as', 'the', 'but', 'or', 'for', 'on', 'at', 'to', 'from', 'by', 'of']\n",
    "print(extract_titles(clean_fandoms[2], stopwords=no_and))\n",
    "print()\n",
    "print(extract_titles(clean_fandoms[5], stopwords=no_and))\n",
    "print()\n",
    "print(extract_titles(clean_fandoms[2]))\n",
    "print()\n",
    "print(extract_titles(clean_fandoms[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doh! \"Quantum Leap\" from the 11th text is messed up! We forgot to drop words from the *end* of the title until we\n",
    "find a capitalized word. For most of the other cases, it didn't matter.\n",
    "What other improvements could you make? What about very short titles or ones that are all\n",
    "in caps? We should filter out the empty lists too. Try and filter out as much of the junk, without losing any real\n",
    "fiction titles.\n",
    "\n",
    "```\n",
    "[['Do'], [], ['Garrison'], ['Gorillas'], ['Harry Potter'], ['Pirates of the CAribbean'], ['Lord of the Rings'], ['Master'], [], ['Smallville'], ['Buffy'], ['Wild Wild West'], ['Wiseguy'], ['Quantum Leap and a'], []]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def improved_find_titles(words, stopwords=shortwords):\n",
    "    # You're on your own here!\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you rate ALL the titles in all the texts by their popularity? Can you use BeautifulSoup to repeat the analysis for the *second* table in the page, about written fiction? Have an open-ended play with the data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
